{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f1b24b0",
   "metadata": {},
   "source": [
    "# tiktoken Module Guide :- \n",
    "- [Doc1](https://www.datacamp.com/tutorial/tiktoken-library-python)\n",
    "- [Doc2](https://pub.dev/documentation/tiktoken/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df83ff53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc3c447",
   "metadata": {},
   "source": [
    "#### Creating Instance for gpt-4 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20ba0e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4_model = tiktoken.encoding_for_model(model_name=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd893dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2654, 277, 939, 374, 264, 50048, 356, 4414]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ID_1 = gpt_4_model.encode(\"Adarsh is a beginner Coder\")\n",
    "token_ID_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faef2b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Adarsh is a beginner Coder'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the Token Ids Back to Text\n",
    "text_1 = gpt_4_model.decode(token_ID_1)\n",
    "text_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30b6a6c",
   "metadata": {},
   "source": [
    "#### Creating Instance for gpt-5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5f17836",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_5_model = tiktoken.encoding_for_model(model_name=\"gpt-5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28b9ad67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2646, 277, 1116, 382, 261, 57062, 363, 6642]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ID_2 = gpt_5_model.encode(\"Adarsh is a beginner Coder\")\n",
    "token_ID_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c39cd29e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Adarsh is a beginner Coder'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the Token Ids Back to Text\n",
    "text_2 = gpt_5_model.decode(token_ID_2)\n",
    "text_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9da3f05",
   "metadata": {},
   "source": [
    "## From Above Two Implementation , We can See that \n",
    "1. each model have different Dictinory or Rules to Determine Rules for Breaking Text Into Tokens\n",
    "2. These Rules  ***impact the efficiency and accuracy of language processing tasks. Different OpenAI models use different encodings.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd11d832",
   "metadata": {},
   "source": [
    "# Important Things Form Todays Video\n",
    "1. According to the video, what is the primary role of the 'Transformer' architecture in an LLM like GPT?\n",
    "> To serve as a special neural network that understands the context of user input and generates new text.\n",
    "2. What is the main purpose of the 'attention mechanism' as described in the video?\n",
    "> To establish relationships between words in the input to understand the correct context.\n",
    "3. What is the fundamental, iterative process that LLMs use to generate responses?\n",
    "> \n",
    "4. Why is the process of tokenization necessary for an LLM?\n",
    "> To convert human-readable text into a numerical format that computers can process.\n",
    "5. What happens immediately after a text input is broken down into tokens?\n",
    "> Each token is assigned a unique number (a token ID) from the model's dictionary.\n",
    "6. Based on the demonstration in the video, what is true about the token IDs generated for the same text by different models like GPT-3 and GPT-4?\n",
    "> The token IDs will be different because each LLM model has its own specific dictionary.\n",
    "7. What does the 'Context Window' of an LLM define?\n",
    "> The maximum amount of data, measured in tokens, that can be processed in a single interaction.\n",
    "8. How are costs typically calculated for using paid LLM services like the OpenAI API?\n",
    "> Based on the total number of tokens for both the input sent to the model and the output received.\n",
    "9. What is the primary reason for defining 'max_tokens' when making a call to an LLM?\n",
    ">  To control and optimize the financial cost of the API call.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a507aa8",
   "metadata": {},
   "source": [
    "# ðŸš€ SENIOR GENAI INTERVIEW CHEATSHEET\n",
    "\n",
    "### 1. Transformer Architecture\n",
    "> **Core Role:** Shifts from sequential (RNN) to parallel processing. Uses **Self-Attention** to model long-range dependencies across the entire sequence simultaneously.\n",
    "\n",
    "### 2. Attention Mechanism\n",
    "> **Function:** Assigns **relevance weights** to input tokens. It determines how much focus the model puts on specific past tokens when predicting the next one, solving the \"vanishing gradient\" problem.\n",
    "\n",
    "### 3. Iterative Generation\n",
    "> **Process:** LLMs are **autoregressive**. They generate one token at a time, calculating a probability distribution for the next token, appending it to the context, and repeating the loop.\n",
    "\n",
    "### 4. Tokenization\n",
    "> **Why:** Neural nets need numbers, not text. Tokenization (via algorithms like **BPE**) breaks text into discrete units to balance vocabulary size and computational efficiency.\n",
    "\n",
    "### 5. Post-Tokenization Step\n",
    "> **Mechanism:** Token IDs are immediately passed to an **Embedding Layer**, converting integers into dense, high-dimensional vectors that encode semantic meaning.\n",
    "\n",
    "### 6. Token ID Consistency\n",
    "> **Verdict:** No. Tokenization is architecture-specific. Different models (GPT-4 vs. LLaMA) use different vocabularies, meaning the same text yields different IDs and counts.\n",
    "\n",
    "### 7. Context Window\n",
    "> **Definition:** The **finite memory buffer** (Input + History + Output) available for a single inference pass. Compute often scales quadratically with this length.\n",
    "\n",
    "### 8. API Cost Calculation\n",
    "> **Formula:** Billing is based on **Total Throughput**: (Prompt/Input Tokens) + (Completion/Output Tokens). Output tokens are usually more expensive (compute-heavy).\n",
    "\n",
    "### 9. max_tokens Parameter\n",
    "> **Purpose:** A production safeguard. It sets a \"hard stop\" on the generation loop to control **latency**, prevent hallucination loops, and manage API budget."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
